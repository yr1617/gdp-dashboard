import streamlit as st
import requests
import xml.etree.ElementTree as ET
from datetime import datetime

# --------------------------------------------------------------------------
# 1. API ì„¤ì • ë° ë°ì´í„° ë¡œë“œ í•¨ìˆ˜ (ë” ì•ˆì •ì ì¸ ë°©ì‹ìœ¼ë¡œ ìˆ˜ì •)
# --------------------------------------------------------------------------

API_KEY = "0b594b395a0248a3a0a68f3b79483427"
BASE_URL = f"https://www.career.go.kr/cnet/openapi/getOpenApi?apiKey={API_KEY}&svcType=api&svcCode=SCHOOL&contentType=xml&gubun=high_list"

@st.cache_data(ttl=3600) # 1ì‹œê°„ ë™ì•ˆ ì „ì²´ ê²°ê³¼ë¥¼ ìºì‹±
def load_all_school_data_robust():
    """
    totalCount ì—†ì´, ë°ì´í„°ê°€ ì—†ì„ ë•Œê¹Œì§€ í˜ì´ì§€ë¥¼ ë„˜ê¸°ë©° ëª¨ë“  ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¤ëŠ” ì•ˆì •ì ì¸ í•¨ìˆ˜
    """
    all_schools = []
    page = 1
    per_page = 100 # í•œ ë²ˆì— 100ê°œì”© ìš”ì²­

    with st.spinner('ì „ì²´ í•™êµ ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘ì…ë‹ˆë‹¤... ì ì‹œë§Œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”.'):
        while True:
            try:
                # í˜„ì¬ í˜ì´ì§€ ë°ì´í„° ìš”ì²­
                response = requests.get(f"{BASE_URL}&perPage={per_page}&page={page}")
                response.raise_for_status()
                root = ET.fromstring(response.content)

                # í˜„ì¬ í˜ì´ì§€ì˜ content ëª©ë¡ì„ ì°¾ìŒ
                contents = root.findall('.//content')
                
                # ë§Œì•½ contentê°€ í•˜ë‚˜ë„ ì—†ë‹¤ë©´, ë§ˆì§€ë§‰ í˜ì´ì§€ë¼ëŠ” ì˜ë¯¸ì´ë¯€ë¡œ ë°˜ë³µ ì¤‘ë‹¨
                if not contents:
                    break

                # ë°ì´í„°ê°€ ìˆë‹¤ë©´ all_schools ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€
                for content in contents:
                    school_info = {
                        'schoolName': content.findtext('schoolName'), 'region': content.findtext('region'),
                        'totalCount': content.findtext('totalCount'), 'major': content.findtext('major'),
                        'subject': content.findtext('subject'), 'chart': content.findtext('chart'),
                        'cert': content.findtext('cert')
                    }
                    all_schools.append(school_info)
                
                # ë‹¤ìŒ í˜ì´ì§€ë¡œ ì´ë™
                page += 1

            except requests.exceptions.RequestException as e:
                st.error(f"API ìš”ì²­ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
                return None
            except ET.ParseError as e:
                st.error(f"XML ë°ì´í„°ë¥¼ íŒŒì‹±í•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
                return None
    
    return all_schools

# --------------------------------------------------------------------------
# 2. Streamlit ì•± UI êµ¬ì„± (ì´í•˜ ë™ì¼)
# --------------------------------------------------------------------------

st.set_page_config(page_title="ì „êµ­ íŠ¹ì„±í™”ê³  í•™ê³¼ ê²€ìƒ‰", page_icon="ğŸ«", layout="wide")

if 'search_history' not in st.session_state:
    st.session_state.search_history = []

st.title("ğŸ« ì „êµ­ íŠ¹ì„±í™”/íŠ¹ìˆ˜ëª©ì  ê³ ë“±í•™êµ í•™ê³¼ ê²€ìƒ‰")

# --- ë°ì´í„° ë¡œë“œ ---
school_data = load_all_school_data_robust()

if school_data is not None:
    st.success(f"âœ… ì´ {len(school_data)}ê°œì˜ í•™êµ ë°ì´í„°ë¥¼ ì„±ê³µì ìœ¼ë¡œ ë¶ˆëŸ¬ì™”ìŠµë‹ˆë‹¤.")
    
    search_query = st.text_input(
        label="ê¶ê¸ˆí•œ í•™ê³¼ í‚¤ì›Œë“œë¥¼ ì…ë ¥í•˜ì„¸ìš” (ì˜ˆ: ë””ìì¸, í”„ë¡œê·¸ë˜ë°, ì¡°ë¦¬)",
        placeholder="ê²€ìƒ‰ì–´ë¥¼ ì…ë ¥í•˜ê³  Enterë¥¼ ëˆ„ë¥´ì„¸ìš”."
    )

    if search_query:
        if search_query not in st.session_state.search_history:
            now = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            st.session_state.search_history.insert(0, f"[{now}] {search_query}")

        results = [school for school in school_data if school['major'] and search_query.lower() in school['major'].lower()]

        st.divider()

        if results:
            st.success(f"'{search_query}'ì— ëŒ€í•œ ê²€ìƒ‰ ê²°ê³¼: ì´ {len(results)}ê±´")
            for item in results:
                with st.expander(f"**{item['schoolName']}** - {item['major']}"):
                    st.markdown(f"**ğŸ« í•™êµëª…:** {item['schoolName']} ({item['region']})")
                    st.markdown(f"**ğŸ“š í•™ê³¼ëª…:** {item['major']}")
                    st.markdown(f"**ğŸ“– ë°°ìš°ëŠ” ë‚´ìš©:** {item['subject']}")
                    st.markdown(f"**ğŸ“ ì¡¸ì—… í›„ ì§„ë¡œ:** {item['chart']}")
                    st.markdown(f"**ğŸ“œ ì·¨ë“ ê°€ëŠ¥ ìê²©ì¦:** {item['cert']}")
        else:
            st.warning(f"'{search_query}'ì— ëŒ€í•œ ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤. ë‹¤ë¥¸ í‚¤ì›Œë“œë¡œ ê²€ìƒ‰í•´ë³´ì„¸ìš”.")
else:
    st.error("ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¤ëŠ”ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ë„¤íŠ¸ì›Œí¬ ìƒíƒœë¥¼ í™•ì¸í•˜ê±°ë‚˜ ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.")


with st.sidebar:
    st.header("ğŸ” ê²€ìƒ‰ ê¸°ë¡")
    if st.session_state.search_history:
        for record in st.session_state.search_history:
            st.text(record)
    else:
        st.info("ì•„ì§ ê²€ìƒ‰ ê¸°ë¡ì´ ì—†ìŠµë‹ˆë‹¤.")
    st.divider()
    st.caption("Powered by Streamlit & CareerNet API")
    