import streamlit as st
import requests
import xml.etree.ElementTree as ET
from datetime import datetime
import time

# --------------------------------------------------------------------------
# 1. API ì„¤ì • ë° ë°ì´í„° ë¡œë“œ í•¨ìˆ˜ (ëª¨ë“  ë°©ì–´ ë¡œì§ì´ í¬í•¨ëœ ìµœì¢…íŒ)
# --------------------------------------------------------------------------

API_KEY = "0b594b395a0248a3a0a68f3b79483427"
BASE_URL = f"https://www.career.go.kr/cnet/openapi/getOpenApi?apiKey={API_KEY}&svcType=api&svcCode=SCHOOL&contentType=xml&gubun=high_list"

@st.cache_data(ttl=3600) # 1ì‹œê°„ ë™ì•ˆ API í˜¸ì¶œ ê²°ê³¼ë¥¼ ìºì‹±
def load_all_school_data_api_definitive():
    """
    ìž¬ì‹œë„, ì¤‘ë³µ ë°ì´í„° ê°ì§€, ìµœì†Œ ë°ì´í„° í™•ì¸ ë¡œì§ì„ ëª¨ë‘ í¬í•¨í•˜ì—¬
    ë¶ˆì•ˆì •í•œ APIì— ìµœëŒ€í•œ ì•ˆì •ì ìœ¼ë¡œ ëŒ€ì‘í•˜ëŠ” ìµœì¢… í•¨ìˆ˜
    """
    all_schools = []
    page = 1
    per_page = 100
    previous_page_content = None # ì´ì „ íŽ˜ì´ì§€ ë‚´ìš©ê³¼ ë¹„êµí•˜ê¸° ìœ„í•œ ë³€ìˆ˜
    
    # --- ìž¬ì‹œë„ ì„¤ì • ---
    MAX_RETRIES = 3 # ìµœëŒ€ 3ë²ˆê¹Œì§€ ìž¬ì‹œë„
    RETRY_DELAY = 1 # ìž¬ì‹œë„ ì‚¬ì´ì— 1ì´ˆ ëŒ€ê¸°

    with st.spinner('API ì„œë²„ë¡œë¶€í„° ì „ì²´ í•™êµ ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘ìž…ë‹ˆë‹¤...'):
        while True:
            is_successful = False
            
            for attempt in range(MAX_RETRIES):
                try:
                    time.sleep(0.1) # ì„œë²„ì— ë¶€ë‹´ì„ ì£¼ì§€ ì•Šê¸° ìœ„í•œ ìµœì†Œí•œì˜ ì§€ì—°
                    response = requests.get(f"{BASE_URL}&perPage={per_page}&page={page}")
                    response.raise_for_status()
                    
                    current_page_content = response.content
                    
                    # [í•µì‹¬ ë¡œì§ 1] ì¤‘ë³µ ë°ì´í„° ê°ì§€ -> ë¬´í•œ ë£¨í”„ ë°©ì§€
                    if current_page_content == previous_page_content:
                        # is_successfulì„ Trueë¡œ ì„¤ì •í•˜ì—¬ ì •ìƒ ì¢…ë£Œë¡œ ì²˜ë¦¬
                        is_successful = True
                        contents = [] # ë£¨í”„ë¥¼ ìžì—°ìŠ¤ëŸ½ê²Œ ì¢…ë£Œì‹œí‚¤ê¸° ìœ„í•´ ë¹ˆ ë¦¬ìŠ¤íŠ¸ë¡œ ì„¤ì •
                        break 
                    
                    root = ET.fromstring(current_page_content)
                    contents = root.findall('.//content')

                    is_successful = True
                    break # ì„±ê³µ ì‹œ ìž¬ì‹œë„ ë£¨í”„ íƒˆì¶œ
                
                except requests.exceptions.RequestException:
                    # ì‹¤íŒ¨ ì‹œ ìž¬ì‹œë„ ì „ ëŒ€ê¸°
                    time.sleep(RETRY_DELAY)
            
            if not is_successful:
                st.error(f"íŽ˜ì´ì§€ {page}ì˜ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¤ëŠ” ë° ìµœì¢… ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. API ì„œë²„ê°€ ë¶ˆì•ˆì •í•œ ê²ƒ ê°™ìŠµë‹ˆë‹¤.")
                return None

            if not contents:
                break # ë°ì´í„°ê°€ ì—†ìœ¼ë©´ ì •ìƒì ìœ¼ë¡œ ì¢…ë£Œ

            for content in contents:
                all_schools.append({
                    'schoolName': content.findtext('schoolName'), 'region': content.findtext('region'),
                    'major': content.findtext('major'), 'subject': content.findtext('subject'),
                    'chart': content.findtext('chart'), 'cert': content.findtext('cert')
                })
            
            previous_page_content = current_page_content
            page += 1
    
    # [í•µì‹¬ ë¡œì§ 2] ìµœì†Œ ë°ì´í„° ê°œìˆ˜ í™•ì¸ -> '1ê°œë§Œ ë°˜í™˜' ë¬¸ì œ ë°©ì–´
    MINIMUM_DATA_THRESHOLD = 10 
    if len(all_schools) < MINIMUM_DATA_THRESHOLD:
        st.error(f"API ì„œë²„ê°€ ë¹„ì •ìƒì ìœ¼ë¡œ ì ì€ ìˆ˜({len(all_schools)}ê°œ)ì˜ ë°ì´í„°ë§Œ ë°˜í™˜í–ˆìŠµë‹ˆë‹¤. ìž ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.")
        return None

    return all_schools

# --------------------------------------------------------------------------
# 2. Streamlit ì•± UI êµ¬ì„± (ì´ì „ê³¼ ë™ì¼)
# --------------------------------------------------------------------------

st.set_page_config(page_title="ì „êµ­ íŠ¹ì„±í™”ê³  í•™ê³¼ ê²€ìƒ‰", page_icon="ðŸ«", layout="wide")

if 'search_history' not in st.session_state:
    st.session_state.search_history = []

st.title("ðŸ« ì „êµ­ íŠ¹ì„±í™”/íŠ¹ìˆ˜ëª©ì  ê³ ë“±í•™êµ í•™ê³¼ ê²€ìƒ‰")

school_data = load_all_school_data_api_definitive()

if school_data:
    st.success(f"âœ… ì´ {len(school_data)}ê°œì˜ í•™ê³¼ ë°ì´í„°ë¥¼ ì„±ê³µì ìœ¼ë¡œ ë¶ˆëŸ¬ì™”ìŠµë‹ˆë‹¤.")
    
    search_query = st.text_input(label="ê¶ê¸ˆí•œ í•™ê³¼ í‚¤ì›Œë“œë¥¼ ìž…ë ¥í•˜ì„¸ìš”", placeholder="ì˜ˆ: ë””ìžì¸, í”„ë¡œê·¸ëž˜ë°, ì¡°ë¦¬")

    if search_query:
        if search_query not in st.session_state.search_history:
            st.session_state.search_history.insert(0, f"[{datetime.now().strftime('%H:%M:%S')}] {search_query}")

        results = [s for s in school_data if s.get('major') and search_query.lower() in s['major'].lower()]

        st.divider()
        if results:
            st.success(f"'{search_query}'ì— ëŒ€í•œ ê²€ìƒ‰ ê²°ê³¼: ì´ {len(results)}ê±´")
            for item in results:
                with st.expander(f"**{item.get('schoolName', 'ì •ë³´ ì—†ìŒ')}** - {item.get('major', 'ì •ë³´ ì—†ìŒ')}"):
                    st.markdown(f"**ðŸ« í•™êµëª…:** {item.get('schoolName', 'ì •ë³´ ì—†ìŒ')} ({item.get('region', 'ì •ë³´ ì—†ìŒ')})")
                    st.markdown(f"**ðŸ“š í•™ê³¼ëª…:** {item.get('major', 'ì •ë³´ ì—†ìŒ')}")
                    st.markdown(f"**ðŸ“– ë°°ìš°ëŠ” ë‚´ìš©:** {item.get('subject', 'ì •ë³´ ì—†ìŒ')}")
                    st.markdown(f"**ðŸŽ“ ì¡¸ì—… í›„ ì§„ë¡œ:** {item.get('chart', 'ì •ë³´ ì—†ìŒ')}")
                    st.markdown(f"**ðŸ“œ ì·¨ë“ ê°€ëŠ¥ ìžê²©ì¦:** {item.get('cert', 'ì •ë³´ ì—†ìŒ')}")
        else:
            st.warning(f"'{search_query}'ì— ëŒ€í•œ ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")

with st.sidebar:
    st.header("ðŸ” ê²€ìƒ‰ ê¸°ë¡")
    st.text_area("ê¸°ë¡:", value="\n".join(st.session_state.search_history), height=300, disabled=True)
    st.caption("Powered by Streamlit & CareerNet API")
    