import streamlit as st
import requests
import xml.etree.ElementTree as ET
from datetime import datetime
import time

# --------------------------------------------------------------------------
# 1. API ì„¤ì • ë° ë°ì´í„° ë¡œë“œ í•¨ìˆ˜ (ì„œìš¸ ì§€ì—­ í•œì •)
# --------------------------------------------------------------------------

API_KEY = "0b594b395a0248a3a0a68f3b79483427"
# [ë³€ê²½ì !] ì„œìš¸ ì§€ì—­ìœ¼ë¡œ ë²”ìœ„ë¥¼ ì¢íˆëŠ” íŒŒë¼ë¯¸í„°(searchRegion=100260)ë¥¼ ì¶”ê°€í–ˆìŠµë‹ˆë‹¤.
BASE_URL = f"https://www.career.go.kr/cnet/openapi/getOpenApi?apiKey={API_KEY}&svcType=api&svcCode=SCHOOL&contentType=xml&gubun=high_list&searchRegion=100260"

@st.cache_data(ttl=3600)
def load_all_school_data_api_definitive():
    # ë‚´ë¶€ ë¡œì§ì€ ì´ì „ì˜ ìµœì¢… API ì½”ë“œì™€ ë™ì¼í•©ë‹ˆë‹¤.
    all_schools = []
    page = 1
    per_page = 100
    previous_page_content = None

    MAX_RETRIES = 3
    RETRY_DELAY = 1

    with st.spinner('API ì„œë²„ë¡œë¶€í„° [ì„œìš¸ ì§€ì—­] í•™êµ ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘ì…ë‹ˆë‹¤...'):
        while True:
            is_successful = False
            for attempt in range(MAX_RETRIES):
                try:
                    time.sleep(0.1)
                    response = requests.get(f"{BASE_URL}&perPage={per_page}&page={page}")
                    response.raise_for_status()
                    
                    current_page_content = response.content
                    
                    if current_page_content == previous_page_content:
                        is_successful = True
                        contents = []
                        break
                    
                    root = ET.fromstring(current_page_content)
                    contents = root.findall('.//content')

                    is_successful = True
                    break
                
                except requests.exceptions.RequestException:
                    time.sleep(RETRY_DELAY)
            
            if not is_successful:
                st.error(f"í˜ì´ì§€ {page}ì˜ ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¤ëŠ” ë° ìµœì¢… ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
                return None

            if not contents:
                break

            for content in contents:
                all_schools.append({
                    'schoolName': content.findtext('schoolName'), 'region': content.findtext('region'),
                    'major': content.findtext('major'), 'subject': content.findtext('subject'),
                    'chart': content.findtext('chart'), 'cert': content.findtext('cert')
                })
            
            previous_page_content = current_page_content
            page += 1
    
    MINIMUM_DATA_THRESHOLD = 5 # ì„œìš¸ ì§€ì—­ì€ ë°ì´í„°ê°€ ì ìœ¼ë¯€ë¡œ ê¸°ì¤€ì¹˜ë¥¼ 5ë¡œ ë‚®ì¶¥ë‹ˆë‹¤.
    if 0 < len(all_schools) < MINIMUM_DATA_THRESHOLD:
        st.error(f"API ì„œë²„ê°€ ë¹„ì •ìƒì ìœ¼ë¡œ ì ì€ ìˆ˜({len(all_schools)}ê°œ)ì˜ ë°ì´í„°ë§Œ ë°˜í™˜í–ˆìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.")
        return None

    return all_schools

# --------------------------------------------------------------------------
# 2. Streamlit ì•± UI êµ¬ì„± (ì´í•˜ ë™ì¼)
# --------------------------------------------------------------------------
# ... (ì´í•˜ UI ì½”ë“œëŠ” ì´ì „ê³¼ ë™ì¼í•˜ë¯€ë¡œ ìƒëµí•©ë‹ˆë‹¤. ê·¸ëŒ€ë¡œ ë‘ì‹œë©´ ë©ë‹ˆë‹¤)
st.set_page_config(page_title="ì „êµ­ íŠ¹ì„±í™”ê³  í•™ê³¼ ê²€ìƒ‰", page_icon="ğŸ«", layout="wide")

if 'search_history' not in st.session_state:
    st.session_state.search_history = []

st.title("ğŸ« ì„œìš¸ íŠ¹ì„±í™”/íŠ¹ìˆ˜ëª©ì  ê³ ë“±í•™êµ í•™ê³¼ ê²€ìƒ‰")

school_data = load_all_school_data_api_definitive()

if school_data:
    st.success(f"âœ… ì´ {len(school_data)}ê°œì˜ í•™ê³¼ ë°ì´í„°ë¥¼ ì„±ê³µì ìœ¼ë¡œ ë¶ˆëŸ¬ì™”ìŠµë‹ˆë‹¤.")
    
    search_query = st.text_input(label="ê¶ê¸ˆí•œ í•™ê³¼ í‚¤ì›Œë“œë¥¼ ì…ë ¥í•˜ì„¸ìš”", placeholder="ì˜ˆ: ë””ìì¸, í”„ë¡œê·¸ë˜ë°, ì¡°ë¦¬")

    if search_query:
        if search_query not in st.session_state.search_history:
            st.session_state.search_history.insert(0, f"[{datetime.now().strftime('%H:%M:%S')}] {search_query}")

        results = [s for s in school_data if s.get('major') and search_query.lower() in s['major'].lower()]

        st.divider()
        if results:
            st.success(f"'{search_query}'ì— ëŒ€í•œ ê²€ìƒ‰ ê²°ê³¼: ì´ {len(results)}ê±´")
            for item in results:
                with st.expander(f"**{item.get('schoolName', 'ì •ë³´ ì—†ìŒ')}** - {item.get('major', 'ì •ë³´ ì—†ìŒ')}"):
                    st.markdown(f"**ğŸ« í•™êµëª…:** {item.get('schoolName', 'ì •ë³´ ì—†ìŒ')} ({item.get('region', 'ì •ë³´ ì—†ìŒ')})")
                    st.markdown(f"**ğŸ“š í•™ê³¼ëª…:** {item.get('major', 'ì •ë³´ ì—†ìŒ')}")
                    st.markdown(f"**ğŸ“– ë°°ìš°ëŠ” ë‚´ìš©:** {item.get('subject', 'ì •ë³´ ì—†ìŒ')}")
                    st.markdown(f"**ğŸ“ ì¡¸ì—… í›„ ì§„ë¡œ:** {item.get('chart', 'ì •ë³´ ì—†ìŒ')}")
                    st.markdown(f"**ğŸ“œ ì·¨ë“ ê°€ëŠ¥ ìê²©ì¦:** {item.get('cert', 'ì •ë³´ ì—†ìŒ')}")
        else:
            st.warning(f"'{search_query}'ì— ëŒ€í•œ ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
elif school_data is None:
     # load_all_school_data_api_definitive í•¨ìˆ˜ì—ì„œ Noneì„ ë°˜í™˜í–ˆì„ ë•Œ (ì˜¤ë¥˜ ë°œìƒ ì‹œ)
     st.warning("ë°ì´í„° ë¡œë”©ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. API ì„œë²„ ìƒíƒœê°€ ë¶ˆì•ˆì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

with st.sidebar:
    st.header("ğŸ” ê²€ìƒ‰ ê¸°ë¡")
    if 'search_history' in st.session_state and st.session_state.search_history:
        st.text_area("ê¸°ë¡:", value="\n".join(st.session_state.search_history), height=300, disabled=True)
    else:
        st.info("ê²€ìƒ‰ ê¸°ë¡ì´ ì—†ìŠµë‹ˆë‹¤.")
    st.caption("Powered by Streamlit & CareerNet API")